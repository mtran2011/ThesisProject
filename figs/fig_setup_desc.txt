stock_trading fig setup description

fig1: OULogStock(price=50, kappa=0.1, mu=log(75), sigma=0.1, tick=1, band=1000)
    lot = 10, actions = tuple(range(-5*lot, 6*lot, lot)), max_holding=10*lot (SMALLER STATE SPACE, ONLY NEED 100K TRAINING)
fig2a, 2b: same as fig1 but max_holding=100*lot (LARGER STATE SPACE, 100k run not enough)
fig3: same as fig2, large state space, 5k training surely not enough
fig4: same as fig3, large state space, 1mil training now works
fig5a,b: same as fig3, 5k train, now try random forest sarsa with m=2, sample=100
fig6a,b,c: same as fig3, 5k train, use random forest but training data = all of Q



option_hedging fig setup description

fig1: spot=50, mu=0, sigma=0.03, tick=1, band=20, strike=50, expiry=53, iv=stock.sigma
    lot=1, actions=range(-5*lot, 6*lot, lot), max_holding=10*lot
    reward = -pnl**2 - transaction_cost
fig2: same as fig1, only increased ntest
fig3: same as fig1, only now we also measure reward not just average reward